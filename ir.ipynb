{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7ce58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from collections import defaultdict\n",
    "from math import log, sqrt\n",
    "from termcolor import colored, cprint\n",
    "print_red_on_cyan = lambda x: cprint(x, 'red', 'on_cyan')\n",
    "\n",
    "\n",
    "inverted_index = defaultdict(list)\n",
    "nos_of_documents = 1553\n",
    "vects_for_docs = []  # we will need nos of docs number of vectors, each vector is a dictionary\n",
    "document_freq_vect = {}  # sort of equivalent to initializing the number of unique words to 0\n",
    "\n",
    "indexed_tokens = []\n",
    "\n",
    "\n",
    "# this is the first function that is executed.\n",
    "# It updates the vects_for_docs variable with vectors of all the documents.\n",
    "def iterate_over_all_docs():\n",
    "    for i in range(nos_of_documents - 1):\n",
    "        doc_text = get_document_text_from_doc_id(i)\n",
    "        token_list = get_tokenized_and_normalized_list(doc_text)\n",
    "        vect = create_vector(token_list)\n",
    "        vects_for_docs.append(vect)\n",
    "                   \n",
    "\n",
    "\n",
    "\n",
    "def get_document_text_from_doc_id(doc_id):\n",
    "    # noinspection PyBroadException\n",
    "    try:\n",
    "        str1 = open(\"corpus/doc\" + str(doc_id).zfill(4)).read()\n",
    "        str1=str1.lower()\n",
    "    except:\n",
    "        str1 = \"\"\n",
    "    return str1\n",
    "\n",
    "\n",
    "\n",
    "# creates a vector from a query in the form of a list (l1) , vector is a dictionary, containing words:frequency pairs\n",
    "def create_vector_from_query(l1):\n",
    "    vect = {}\n",
    "    for token in l1:\n",
    "        if token in vect:\n",
    "            vect[token] += 1.0\n",
    "        else:\n",
    "            vect[token] = 1.0\n",
    "    return vect\n",
    "\n",
    "\n",
    "# name is self explanatory, it generates and inverted index in the global variable inverted_index,\n",
    "# however, precondition is that vects_for_docs should be completely initialized\n",
    "def generate_inverted_index():\n",
    "    count1 = 0\n",
    "    for vector in vects_for_docs:\n",
    "        for word1 in vector:\n",
    "            inverted_index[word1].append(count1)\n",
    "        count1 += 1\n",
    "\n",
    "\n",
    "# it updates the vects_for_docs global variable (the list of frequency vectors for all the documents)\n",
    "# and changes all the frequency vectors to tf-idf unit vectors (tf-idf score instead of frequency of the words)\n",
    "def create_tf_idf_vector():\n",
    "    vect_length = 0.0\n",
    "    for vect in vects_for_docs:\n",
    "        for word1 in vect:\n",
    "            word_freq = vect[word1]\n",
    "            temp = calc_tf_idf(word1, word_freq)\n",
    "            vect[word1] = temp\n",
    "            vect_length += temp ** 2\n",
    "\n",
    "        vect_length = sqrt(vect_length)\n",
    "        for word1 in vect:\n",
    "            vect[word1] /= vect_length\n",
    "\n",
    "\n",
    "# note: even though you do not need to convert the query vector into a unit vector,\n",
    "# I have done so because that would make all the dot products <= 1\n",
    "# as the name suggests, this function converts a given query vector\n",
    "# into a tf-idf unit vector(word:tf-idf vector given a word:frequency vector\n",
    "def get_tf_idf_from_query_vect(query_vector1):\n",
    "    vect_length = 0.0\n",
    "    for word1 in query_vector1:\n",
    "        word_freq = query_vector1[word1]\n",
    "        if word1 in document_freq_vect:  # I have left out any term which has not occurred in any document because\n",
    "            query_vector1[word1] = calc_tf_idf(word1, word_freq)\n",
    "        else:\n",
    "            query_vector1[word1] = log(1 + word_freq) * log(\n",
    "                nos_of_documents)  # this additional line will ensure that if the 2 queries,\n",
    "            # the first having all words in some documents,\n",
    "            #   and the second having and extra word that is not in any document,\n",
    "            # will not end up having the same dot product value for all documents\n",
    "        vect_length += query_vector1[word1] ** 2\n",
    "    vect_length = sqrt(vect_length)\n",
    "    if vect_length != 0:\n",
    "        for word1 in query_vector1:\n",
    "            query_vector1[word1] /= vect_length\n",
    "\n",
    "\n",
    "# precondition: word is in the document_freq_vect\n",
    "# this function calculates the tf-idf score for a given word in a document\n",
    "def calc_tf_idf(word1, word_freq):\n",
    "    return log(1 + word_freq) * log(nos_of_documents / document_freq_vect[word1])\n",
    "\n",
    "\n",
    "# define a number of functions,\n",
    "# function to to read a given document word by word and\n",
    "# 1. Start building the dictionary of the word frequency of the document,\n",
    "#       2. Update the number of distinct words\n",
    "#  function to :\n",
    "#       1. create the dictionary of the term freqency (number of documents which have the terms);\n",
    "\n",
    "\n",
    "# this function returns the dot product of vector1 and vector2\n",
    "def get_dot_product(vector1, vector2):\n",
    "    if len(vector1) > len(vector2):  # this will ensure that len(dict1) < len(dict2)\n",
    "        temp = vector1\n",
    "        vector1 = vector2\n",
    "        vector2 = temp\n",
    "    keys1 = vector1.keys()\n",
    "    keys2 = vector2.keys()\n",
    "    sum = 0\n",
    "    for i in keys1:\n",
    "        if i in keys2:\n",
    "            sum += vector1[i] * vector2[i]\n",
    "    return sum\n",
    "\n",
    "\n",
    "# this function returns a list of tokenized and stemmed words of any text\n",
    "def get_tokenized_and_normalized_list(doc_text):\n",
    "    # return doc_text.split()\n",
    "\n",
    "\n",
    "    tokens = nltk.word_tokenize(doc_text)\n",
    "    ps = nltk.stem.PorterStemmer()\n",
    "    stemmed = []\n",
    "    for words in tokens:\n",
    "        stemmed.append(ps.stem(words))\n",
    "    return stemmed\n",
    "\n",
    "\n",
    "# creates a vector from a list (l1) , vector is a dictionary, containing words:frequency pairs\n",
    "# this function should not be called to parse the query given by the user\n",
    "# because this function also updates the document frequency dictionary\n",
    "def create_vector(l1):\n",
    "    vect = {}  # this is a dictionary\n",
    "    global document_freq_vect\n",
    "\n",
    "    for token in l1:\n",
    "        if token in vect:\n",
    "            vect[token] += 1\n",
    "        else:\n",
    "            vect[token] = 1\n",
    "            if token in document_freq_vect:\n",
    "                document_freq_vect[token] += 1\n",
    "            else:\n",
    "                document_freq_vect[token] = 1\n",
    "    return vect\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# this function takes the dot product of the query with all the documents\n",
    "#  and returns a sorted list of tuples of docId, cosine score pairs\n",
    "def get_result_from_query_vect(query_vector1):\n",
    "    parsed_list = []\n",
    "    for i in range(nos_of_documents - 1):\n",
    "        dot_prod = get_dot_product(query_vector1, vects_for_docs[i])\n",
    "        parsed_list.append((i, dot_prod))\n",
    "        parsed_list = sorted(parsed_list, key=lambda x: x[1])\n",
    "    return parsed_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# now the actual execution starts (this is equivalent to the main function of java)\n",
    "\n",
    "\n",
    "# initializing the vects_for_docs variable\n",
    "iterate_over_all_docs()\n",
    "\n",
    "# self explanatory\n",
    "generate_inverted_index()\n",
    "\n",
    "# changes the frequency values in vects_for_docs to tf-idf values\n",
    "create_tf_idf_vector()\n",
    "\n",
    "print()\n",
    "while True:\n",
    "    query = input(\"Please enter your query....\\n\").lower()\n",
    "    if len(query) == 0:\n",
    "        break\n",
    "    query_list = get_tokenized_and_normalized_list(query)\n",
    "    query_vector = create_vector_from_query(query_list)\n",
    "    get_tf_idf_from_query_vect(query_vector)\n",
    "    result_set = get_result_from_query_vect(query_vector)\n",
    "\n",
    "    counter = 0;\n",
    "\n",
    "    for tup in reversed(result_set):\n",
    "        if tup[1] > 0.00000000000 and counter < 10  :\n",
    "\n",
    "            print_red_on_cyan(\"RESULT NO.\"+ str(counter+1) + \":\")\n",
    "            print(\"The documentId is \" + str(tup[0]).zfill(4) + \" and the weight(most hits acc. to query) is \" + str(tup[1]))\n",
    "            print(\"line numbers of where match hits are:- \")\n",
    "            f = open(\"corpus/doc\" + str(tup[0]).zfill(4))\n",
    "            i = 0\n",
    "            list_of_lines = []\n",
    "            for line in f:\n",
    "                line=line.lower()\n",
    "                i+=1\n",
    "                if any(word in line for word in query_list):\n",
    "                    list_of_lines.append(i)\n",
    "            counter+=1\n",
    "            print(list_of_lines[:10])\n",
    "            print()\n",
    "        if counter >= 10:\n",
    "            break    \n",
    "\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c05022",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36255f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
